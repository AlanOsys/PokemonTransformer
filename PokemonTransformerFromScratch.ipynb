{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "Learning_rate = 3e-4\n",
    "Max_iterations = 5000\n",
    "Evaluation_iterations = 100\n",
    "Evaluation_Intervals = 50\n",
    "Batch_size = 64\n",
    "\n",
    "\n",
    "Block_size = 15\n",
    "Embedding_neurons = 400\n",
    "#Head_Size = 3\n",
    "Layers_amount = 9\n",
    "Number_Heads = 7\n",
    "\n",
    "\n",
    "\n",
    "Temperature = 1.2\n",
    "dropout = 0.2\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save(model,file_name='FromScratchModel.pth'):\n",
    "    model_folder_path = './model'\n",
    "    if not os.path.exists(model_folder_path):\n",
    "        os.makedirs(model_folder_path)\n",
    "    file_name = os.path.join(model_folder_path, file_name)\n",
    "    torch.save(model.state_dict(),file_name)\n",
    "    \n",
    "def load(file_name='FromScratchModel.pth'):\n",
    "    model_folder_path = './model'\n",
    "    \n",
    "    file_name = os.path.join(model_folder_path, file_name)\n",
    "    return file_name\n",
    "    #torch.save(self.state_dict(),file_name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0, '0': 1, '1': 2, '10': 3, '100': 4, '104': 5, '105': 6, '106': 7, '107': 8, '108': 9, '112': 10, '114': 11, '115': 12, '116': 13, '118': 14, '12': 15, '120': 16, '123': 17, '124': 18, '125': 19, '126': 20, '128': 21, '130': 22, '131': 23, '132': 24, '133': 25, '136': 26, '138': 27, '14': 28, '140': 29, '142': 30, '144': 31, '148': 32, '149': 33, '150': 34, '152': 35, '156': 36, '158': 37, '16': 38, '160': 39, '163': 40, '164': 41, '168': 42, '170': 43, '171': 44, '172': 45, '174': 46, '175': 47, '176': 48, '177': 49, '180': 50, '184': 51, '186': 52, '187': 53, '188': 54, '190': 55, '191': 56, '192': 57, '196': 58, '198': 59, '2': 60, '20': 61, '200': 62, '204': 63, '208': 64, '210': 65, '211': 66, '212': 67, '216': 68, '220': 69, '224': 70, '228': 71, '230': 72, '232': 73, '234': 74, '236': 75, '239': 76, '24': 77, '240': 78, '242': 79, '244': 80, '245': 81, '248': 82, '251': 83, '252': 84, '28': 85, '3': 86, '30': 87, '31': 88, '32': 89, '36': 90, '39': 91, '4': 92, '40': 93, '41': 94, '42': 95, '44': 96, '46': 97, '48': 98, '5': 99, '50': 100, '52': 101, '54': 102, '55': 103, '56': 104, '58': 105, '6': 106, '60': 107, '64': 108, '66': 109, '68': 110, '69': 111, '70': 112, '72': 113, '76': 114, '77': 115, '78': 116, '8': 117, '80': 118, '81': 119, '83': 120, '84': 121, '85': 122, '86': 123, '88': 124, '89': 125, '92': 126, '94': 127, '96': 128, '97': 129, '98': 130, 'Ability': 131, 'Absorb': 132, 'Accelerock': 133, 'Acid Armor': 134, 'Acid Spray': 135, 'Acrobatics': 136, 'Adamant': 137, 'Adaptability': 138, 'Adrenaline Orb': 139, 'Aerial Ace': 140, 'Aeroblast': 141, 'After You': 142, 'Aguav Berry': 143, 'Air Balloon': 144, 'Air Lock': 145, 'Air Slash': 146, 'Ally Switch': 147, 'Analytic': 148, 'Ancient Power': 149, 'Anger Point': 150, 'Apple Acid': 151, 'Aqua Jet': 152, 'Aqua Tail': 153, 'Arena Trap': 154, 'Aroma Veil': 155, 'Assault Vest': 156, 'Astral Barrage': 157, 'Atk': 158, 'Aura Sphere': 159, 'Aura Wheel': 160, 'Aurora Veil': 161, 'Babiri Berry': 162, 'Baby Doll Eyes': 163, 'Baneful Bunker': 164, 'Bashful': 165, 'Baton Pass': 166, 'Battery': 167, 'Battle Armor': 168, 'Beast Boost': 169, 'Behemoth Bash': 170, 'Behemoth Blade': 171, 'Belly Drum': 172, 'Berserk': 173, 'Bite': 174, 'Black Sludge': 175, 'Blast Burn': 176, 'Blizzard': 177, 'Blue Flare': 178, 'Body Press': 179, 'Body Slam': 180, 'Bold': 181, 'Bolt Beak': 182, 'Bonemerang': 183, 'Boomburst': 184, 'Bounce': 185, 'Brave': 186, 'Brave Bird': 187, 'Breaking Swipe': 188, 'Brick Break': 189, 'Brutal Swing': 190, 'Bug': 191, 'Bug Bite': 192, 'Bug Buzz': 193, 'Bulk Up': 194, 'Bulldoze': 195, 'Bullet Punch': 196, 'Bullet Seed': 197, 'Burn Up': 198, 'Burning Jealousy': 199, 'Calm': 200, 'Calm Mind': 201, 'Careful': 202, 'Charcoal': 203, 'Charm': 204, 'Cheek Pouch': 205, 'Chilling Neigh': 206, 'Chlorophyll': 207, 'Choice Band': 208, 'Choice Scarf': 209, 'Choice Specs': 210, 'Clanging Scales': 211, 'Clear Body': 212, 'Clear Smog': 213, 'Close Combat': 214, 'Cloud Nine': 215, 'Coaching': 216, 'Coba Berry': 217, 'Coil': 218, 'Colbur Berry': 219, 'Competitive': 220, 'Compound Eyes': 221, 'Contrary': 222, 'Copycat': 223, 'Corrosion': 224, 'Cosmic Power': 225, 'Cotton Down': 226, 'Cotton Guard': 227, 'Counter': 228, 'Crabhammer': 229, 'Crunch': 230, 'Curse': 231, 'Cursed Body': 232, 'Custap Berry': 233, 'Dark': 234, 'Dark Aura': 235, 'Dark Pulse': 236, 'Darkest Lariat': 237, 'Dauntless Shield': 238, 'Dazzling Gleam': 239, 'Def': 240, 'Defiant': 241, 'Destiny Bond': 242, 'Detect': 243, 'Dig': 244, 'Discharge': 245, 'Disguise': 246, 'Docile': 247, 'Double Edge': 248, 'Download': 249, 'Draco Meteor': 250, 'Dragon': 251, 'Dragon Ascent': 252, 'Dragon Claw': 253, 'Dragon Dance': 254, 'Dragon Darts': 255, 'Dragon Energy': 256, 'Dragon Hammer': 257, 'Dragon Pulse': 258, 'Dragon Rush': 259, 'Drain Punch': 260, 'Draining Kiss': 261, 'Drill Run': 262, 'Drizzle': 263, 'Drought': 264, 'Dry Skin': 265, 'Dual Wingbeat': 266, 'Dynamax Cannon': 267, 'Dynamic Punch': 268, 'Earth Power': 269, 'Earthquake': 270, 'Eerie Impulse': 271, 'Effect Spore': 272, 'Eject Button': 273, 'Eject Pack': 274, 'Electric': 275, 'Electric Surge': 276, 'Electrify': 277, 'Electro Ball': 278, 'Electroweb': 279, 'Emergency Exit': 280, 'Encore': 281, 'Endeavor': 282, 'Endure': 283, 'Energy Ball': 284, 'Entrainment': 285, 'Eruption': 286, 'Eviolite': 287, 'Expanding Force': 288, 'Expert Belt': 289, 'Explosion': 290, 'Extrasensory': 291, 'Extreme Speed': 292, 'Facade': 293, 'Fairy': 294, 'Fairy Aura': 295, 'Fake Out': 296, 'Fake Tears': 297, 'Feint': 298, 'Fiery Wrath': 299, 'Fighting': 300, 'Fire': 301, 'Fire Blast': 302, 'Fire Fang': 303, 'Fire Punch': 304, 'First Impression': 305, 'Fishious Rend': 306, 'Flame Body': 307, 'Flame Orb': 308, 'Flamethrower': 309, 'Flare Blitz': 310, 'Flash Cannon': 311, 'Flash Fire': 312, 'Fling': 313, 'Flip Turn': 314, 'Floral Healing': 315, 'Flower Gift': 316, 'Fluffy': 317, 'Fly': 318, 'Flying': 319, 'Focus Blast': 320, 'Focus Energy': 321, 'Focus Sash': 322, 'Follow Me': 323, 'Forests Curse': 324, 'Foul Play': 325, 'Freeze Dry': 326, 'Freezing Glare': 327, 'Friend Guard': 328, 'Frisk': 329, 'Full Metal Body': 330, 'Fur Coat': 331, 'Fusion Bolt': 332, 'Gale Wings': 333, 'Gear Grind': 334, 'Gentle': 335, 'Geomancy': 336, 'Ghost': 337, 'Giga Drain': 338, 'Giga Impact': 339, 'Glacial Lance': 340, 'Glare': 341, 'Gorilla Tactics': 342, 'Grass': 343, 'Grass Knot': 344, 'Grass Pledge': 345, 'Grassy Glide': 346, 'Grassy Surge': 347, 'Grav Apple': 348, 'Gravity': 349, 'Grim Neigh': 350, 'Griseous Orb': 351, 'Ground': 352, 'Guard Split': 353, 'Gunk Shot': 354, 'Guts': 355, 'Gyro Ball': 356, 'HP': 357, 'Hammer Arm': 358, 'Hardy': 359, 'Harvest': 360, 'Hasty': 361, 'Haze': 362, 'Head Charge': 363, 'Head Smash': 364, 'Heal Pulse': 365, 'Healer': 366, 'Heat Wave': 367, 'Heavy Slam': 368, 'Helping Hand': 369, 'High Horsepower': 370, 'High Jump Kick': 371, 'Horn Drill': 372, 'Howl': 373, 'Huge Power': 374, 'Hunger Switch': 375, 'Hurricane': 376, 'Hustle': 377, 'Hydro Cannon': 378, 'Hydro Pump': 379, 'Hyper Beam': 380, 'Hyper Cutter': 381, 'Hyper Voice': 382, 'Hypnosis': 383, 'Iapapa Berry': 384, 'Ice': 385, 'Ice Beam': 386, 'Ice Face': 387, 'Ice Fang': 388, 'Ice Punch': 389, 'Ice Shard': 390, 'Icicle Crash': 391, 'Icicle Spear': 392, 'Icy Wind': 393, 'Illusion': 394, 'Impish': 395, 'Imposter': 396, 'Imprison': 397, 'Inner Focus': 398, 'Instruct': 399, 'Intimidate': 400, 'Intrepid Sword': 401, 'Iron Ball': 402, 'Iron Barbs': 403, 'Iron Defense': 404, 'Iron Head': 405, 'Iron Tail': 406, 'Item': 407, 'Jolly': 408, 'Justified': 409, 'Kings Shield': 410, 'Klutz': 411, 'Knock Off': 412, 'Lansat Berry': 413, 'Lash Out': 414, 'Lax': 415, 'Leaf Blade': 416, 'Leaf Storm': 417, 'Leech Life': 418, 'Leech Seed': 419, 'Leek': 420, 'Leer': 421, 'Leftovers': 422, 'Levitate': 423, 'Libero': 424, 'Life Dew': 425, 'Life Orb': 426, 'Light Ball': 427, 'Light Clay': 428, 'Light Screen': 429, 'Lightning Rod': 430, 'Limber': 431, 'Liquid Voice': 432, 'Liquidation': 433, 'Lonely': 434, 'Low Kick': 435, 'Lum Berry': 436, 'Magic Bounce': 437, 'Magic Guard': 438, 'Mago Berry': 439, 'Mega Launcher': 440, 'Megahorn': 441, 'Memento': 442, 'Mental Herb': 443, 'Meteor Beam': 444, 'Mild': 445, 'Mind Blown': 446, 'Minimize': 447, 'Mirror Armor': 448, 'Misty Surge': 449, 'Modest': 450, 'Mold Breaker': 451, 'Moonblast': 452, 'Moongeist Beam': 453, 'Moonlight': 454, 'Motor Drive': 455, 'Move': 456, 'Moxie': 457, 'Mud Shot': 458, 'Muddy Water': 459, 'Multiscale': 460, 'Mummy': 461, 'Mystic Water': 462, 'Mystical Fire': 463, 'Naive': 464, 'Nasty Plot': 465, 'Natural Cure': 466, 'Nature': 467, 'Natures Madness': 468, 'Naughty': 469, 'Neutralizing Gas': 470, 'Night Shade': 471, 'Night Slash': 472, 'No Guard': 473, 'No Retreat': 474, 'None': 475, 'Normal': 476, 'Nuzzle': 477, 'Oblivion Wing': 478, 'Oblivious': 479, 'Occa Berry': 480, 'Octolock': 481, 'Origin Pulse': 482, 'Outrage': 483, 'Overcoat': 484, 'Overdrive': 485, 'Overgrow': 486, 'Overheat': 487, 'Pain Split': 488, 'Parting Shot': 489, 'Perish Body': 490, 'Perish Song': 491, 'Phantom Force': 492, 'Pixie Plate': 493, 'Pixilate': 494, 'Play Rough': 495, 'Pluck': 496, 'Poison': 497, 'Poison Jab': 498, 'Pollen Puff': 499, 'Poltergeist': 500, 'Power Construct': 501, 'Power Gem': 502, 'Power Herb': 503, 'Power Lens': 504, 'Power Split': 505, 'Power Spot': 506, 'Power Trip': 507, 'Power Whip': 508, 'Prankster': 509, 'Precipice Blades': 510, 'Pressure': 511, 'Prism Armor': 512, 'Protect': 513, 'Psychic': 514, 'Psychic Fangs': 515, 'Psychic Seed': 516, 'Psychic Surge': 517, 'Psycho Cut': 518, 'Psyshock': 519, 'Psystrike': 520, 'Punk Rock': 521, 'Pyro Ball': 522, 'Quash': 523, 'Queenly Majesty': 524, 'Quick Attack': 525, 'Quick Draw': 526, 'Quick Guard': 527, 'Quiet': 528, 'Quirky': 529, 'Rage Powder': 530, 'Rain Dance': 531, 'Rapid Spin': 532, 'Rash': 533, 'Razor Claw': 534, 'Recover': 535, 'Reflect': 536, 'Regenerator': 537, 'Relaxed': 538, 'Retaliate': 539, 'Rindo Berry': 540, 'Rising Voltage': 541, 'Roar': 542, 'Roar Of Time': 543, 'Rock': 544, 'Rock Blast': 545, 'Rock Head': 546, 'Rock Polish': 547, 'Rock Slide': 548, 'Rock Tomb': 549, 'Rock Wrecker': 550, 'Rocky Helmet': 551, 'Room Service': 552, 'Roost': 553, 'Rough Skin': 554, 'Round': 555, 'Rusted Shield': 556, 'Rusted Sword': 557, 'Sacred Fire': 558, 'Sacred Sword': 559, 'Safeguard': 560, 'Safety Goggles': 561, 'Sand Rush': 562, 'Sand Stream': 563, 'Sap Sipper': 564, 'Sassy': 565, 'Scald': 566, 'Schooling': 567, 'Scope Lens': 568, 'Scrappy': 569, 'Screech': 570, 'Screen Cleaner': 571, 'Seed Bomb': 572, 'Seismic Toss': 573, 'Serene Grace': 574, 'Serious': 575, 'Shadow Ball': 576, 'Shadow Bone': 577, 'Shadow Claw': 578, 'Shadow Force': 579, 'Shadow Shield': 580, 'Shadow Sneak': 581, 'Shadow Tag': 582, 'Sharp Beak': 583, 'Shed Skin': 584, 'Sheer Force': 585, 'Shell Armor': 586, 'Shell Side Arm': 587, 'Shell Smash': 588, 'Shell Trap': 589, 'Shield Dust': 590, 'Shift Gear': 591, 'Shore Up': 592, 'Shuca Berry': 593, 'Simple': 594, 'Sing': 595, 'Sitrus Berry': 596, 'Skill Link': 597, 'Skill Swap': 598, 'Skitter Smack': 599, 'Slack Off': 600, 'Sleep Powder': 601, 'Sleep Talk': 602, 'Slow Start': 603, 'Sludge Bomb': 604, 'Sludge Wave': 605, 'Slush Rush': 606, 'Smart Strike': 607, 'Snarl': 608, 'Snipe Shot': 609, 'Sniper': 610, 'Snow Cloak': 611, 'Snow Warning': 612, 'Soak': 613, 'Soft Boiled': 614, 'Solar Beam': 615, 'Solar Blade': 616, 'Solar Power': 617, 'Solid Rock': 618, 'Soundproof': 619, 'SpA': 620, 'SpD': 621, 'Spacial Rend': 622, 'Spe': 623, 'Speed Boost': 624, 'Speed Swap': 625, 'Spiky Shield': 626, 'Spirit Break': 627, 'Spore': 628, 'Stalwart': 629, 'Stance Change': 630, 'Static': 631, 'Stealth Rock': 632, 'Steam Engine': 633, 'Steel': 634, 'Steel Beam': 635, 'Steel Roller': 636, 'Steel Wing': 637, 'Steelworker': 638, 'Steely Spirit': 639, 'Stockpile': 640, 'Stomping Tantrum': 641, 'Stone Edge': 642, 'Stored Power': 643, 'Storm Drain': 644, 'Strange Steam': 645, 'Strength Sap': 646, 'Strong Jaw': 647, 'Struggle Bug': 648, 'Sturdy': 649, 'Substitute': 650, 'Sucker Punch': 651, 'Sunny Day': 652, 'Sunsteel Strike': 653, 'Super Luck': 654, 'Superpower': 655, 'Surf': 656, 'Surge Surfer': 657, 'Surging Strikes': 658, 'Swagger': 659, 'Swift Swim': 660, 'Switcheroo': 661, 'Swords Dance': 662, 'Synthesis': 663, 'Tail Slap': 664, 'Tail Whip': 665, 'Tailwind': 666, 'Taunt': 667, 'Tearful Look': 668, 'Teatime': 669, 'Technician': 670, 'Telepathy': 671, 'Teravolt': 672, 'Terrain Pulse': 673, 'Thick Club': 674, 'Thick Fat': 675, 'Thousand Arrows': 676, 'Throat Chop': 677, 'Thunder': 678, 'Thunder Punch': 679, 'Thunder Wave': 680, 'Thunderbolt': 681, 'Thunderous Kick': 682, 'Timid': 683, 'Tinted Lens': 684, 'Torrent': 685, 'Tough Claws': 686, 'Toxic': 687, 'Toxic Orb': 688, 'Trace': 689, 'Transform': 690, 'Transistor': 691, 'Tri Attack': 692, 'Triage': 693, 'Trick Or Treat': 694, 'Trick Room': 695, 'Triple Axel': 696, 'Turboblaze': 697, 'U Turn': 698, 'Unaware': 699, 'Unburden': 700, 'Unnerve': 701, 'Unseen Fist': 702, 'Venoshock': 703, 'Vital Spirit': 704, 'Volt Absorb': 705, 'Volt Switch': 706, 'Volt Tackle': 707, 'Wacan Berry': 708, 'Wandering Spirit': 709, 'Water': 710, 'Water Absorb': 711, 'Water Bubble': 712, 'Water Compaction': 713, 'Water Pulse': 714, 'Water Shuriken': 715, 'Water Spout': 716, 'Waterfall': 717, 'Weak Armor': 718, 'Weakness Policy': 719, 'Weather Ball': 720, 'White Herb': 721, 'White Smoke': 722, 'Wicked Blow': 723, 'Wide Guard': 724, 'Wide Lens': 725, 'Wild Charge': 726, 'Will O Wisp': 727, 'Wonder Guard': 728, 'Wood Hammer': 729, 'X Scissor': 730, 'Yawn': 731, 'Zen Headbutt': 732}\n",
      "294\n",
      "91214\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('PokemonStatsFile.txt', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    lines = f.readlines()\n",
    "    random.shuffle(lines)\n",
    "f.close()\n",
    "\n",
    "open('PokemonStatsFile.txt', 'w').writelines(lines)\n",
    "with open('PokemonStatsFile.txt', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    text = f.read().replace(\"\\n\",\"\")\n",
    "#print(text)\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text.split(\",\"))))#+sorted(list(set(text)))\n",
    "\n",
    "Alphabet = chars\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "print(stoi)\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ' '.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "print(stoi['Fairy'])\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text.split(\",\")), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - Block_size, (Batch_size,))\n",
    "    x = torch.stack([data[i:i+Block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+Block_size+1] for i in ix])\n",
    "    #print(x,len(y))\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    #Loss output\n",
    "    out = {}\n",
    "    #Model evaluation\n",
    "    model.eval()\n",
    "    #iterate through the model first using training data then evaluation data\n",
    "    for split in ['train', 'val']:\n",
    "        #initiate a losses array\n",
    "        losses = []\n",
    "        #loop evalutation for however many iterations\n",
    "        for k in range(Evaluation_iterations):\n",
    "            #generates the inputs and the target outputs\n",
    "            X, Y = get_batch(split)\n",
    "            #runs the X inputs through the network and compares them to the Y outputs to generate loss\n",
    "            logits, loss = model(X, Y)\n",
    "            #appends the loss to an array\n",
    "            losses.append(loss.item())\n",
    "        #saves the train and validation average losses seperately \n",
    "        out[split] = np.array(losses).mean()\n",
    "    #trains the model, TODO why\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadOfAttention(nn.Module):\n",
    "    def __init__(self, Head_Size):\n",
    "        super().__init__()\n",
    "        self.Query = nn.Linear(Embedding_neurons,Head_Size, bias=False)\n",
    "        self.Key = nn.Linear(Embedding_neurons,Head_Size, bias=False)\n",
    "        self.Value = nn.Linear(Embedding_neurons,Head_Size, bias=False)\n",
    "        self.register_buffer(\"tril\",torch.tril(torch.ones(Block_size, Block_size)))\n",
    "        self.Dropout = nn.Dropout(dropout)\n",
    "    def forward(self,x):\n",
    "        #get BTC from the input\n",
    "        Batch,TimeStep,Channel = x.shape\n",
    "        #Key\n",
    "        K = self.Key(x)\n",
    "        #Query\n",
    "        Q = self.Query(x)\n",
    "        #Obtaining W from performing a matrix multiplication of the Querys and Keys\n",
    "        Wqk = Q @ K.transpose(-2,-1) * K.shape[-1]**-0.5\n",
    "        #Applying a mask\n",
    "        Wqk = Wqk.masked_fill(self.tril[:TimeStep, :TimeStep] == 0, float('-inf'))\n",
    "        #Performing a softmax to obtain the probabilities of each tokens likelyhood of following what has been inputted\n",
    "        Wqk = F.softmax(Wqk.to(device), dim=-1)\n",
    "        #apply dropout to W\n",
    "        Wqk = self.Dropout(Wqk)\n",
    "        #Obtaining the value\n",
    "        V = self.Value(x)\n",
    "        #Generating an output\n",
    "        output = Wqk @ V\n",
    "\n",
    "        return output\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self,Number_Heads,Head_Size):\n",
    "        super().__init__()\n",
    "        #initialise heads of attention\n",
    "        self.AttentionHeads = nn.ModuleList([HeadOfAttention(Head_Size) for _ in range(Number_Heads)])\n",
    "        #Linear projections\n",
    "        self.Projection = nn.Linear(Head_Size * Number_Heads, Embedding_neurons)\n",
    "        #dropout\n",
    "        self.Dropout = nn.Dropout(dropout)\n",
    "    def forward(self,x):\n",
    "        #concatonating the heads of attention for the output\n",
    "        out = torch.cat([h(x) for h in self.AttentionHeads], dim=-1)\n",
    "        #output is put through a dropout to prevent overfitting\n",
    "        out = self.Dropout(self.Projection(out))\n",
    "        return out\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,Embedding_neurons):\n",
    "        super().__init__()\n",
    "        #Initialising FeedForward network\n",
    "        self.Network = nn.Sequential(\n",
    "            nn.Linear(Embedding_neurons,4*Embedding_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*Embedding_neurons,Embedding_neurons),\n",
    "            nn.Dropout(dropout)\n",
    "\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.Network(x)\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, Number_Embeds, Number_Heads):\n",
    "        super().__init__()\n",
    "        #Calculating the head_size\n",
    "        Head_Size = Number_Embeds // Number_Heads\n",
    "        \n",
    "        self.SelfAttention = MultiHeadedAttention(Number_Heads,Head_Size)\n",
    "        self.FeedForward = FeedForward(Number_Embeds)\n",
    "        self.LinearNormalisation1 = nn.LayerNorm(Number_Embeds)\n",
    "        self.LinearNormalisation2 = nn.LayerNorm(Number_Embeds)\n",
    "    def forward(self,x):\n",
    "        #the input is the input concatonated with self attention that had the input linearly normalised as its input\n",
    "        x = x + self.SelfAttention(self.LinearNormalisation1(x))\n",
    "        x = x + self.FeedForward(self.LinearNormalisation2(x))\n",
    "        return x\n",
    "\n",
    "class LLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Token_Embedding_Table = nn.Embedding(vocab_size,Embedding_neurons)\n",
    "        self.Positional_Embedding_Table = nn.Embedding(Block_size,Embedding_neurons)\n",
    "        self.Blocks = nn.Sequential(*[Block(Embedding_neurons, Number_Heads) for _ in range(Layers_amount)])\n",
    "        self.LayerNormalisation = nn.LayerNorm(Embedding_neurons) # final layer norm\n",
    "        self.Linear_Head = nn.Linear(Embedding_neurons, vocab_size)\n",
    "        #Initialise weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    def forward(self,index,targets=None):\n",
    "        Batch, TimeStep = index.shape\n",
    "\n",
    "        #Parse the indexes of the input through the Token embedding and positional embedding to be learnt\n",
    "        Token_Embedding = self.Token_Embedding_Table(index)\n",
    "        Positional_Embedding = self.Positional_Embedding_Table(torch.arange(TimeStep, device=device))\n",
    "        #concatonate the Token and Positional Embeddings to create the input which we then parse through our network\n",
    "        x = Token_Embedding + Positional_Embedding\n",
    "        x = self.Blocks(x)\n",
    "        x = self.LayerNormalisation(x)\n",
    "        Logits = self.Linear_Head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            Batch, TimeStep, Channel = Logits.shape\n",
    "            Logits = Logits.view(Batch*TimeStep, Channel)\n",
    "            targets = targets.view(Batch*TimeStep)\n",
    "            #calculate a cross entropy loss between the logits that were created from parsing the input through the linear layer and the target output\n",
    "            loss = F.cross_entropy(Logits, targets)\n",
    "\n",
    "        return Logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            #only look at the indexes that are within our block size\n",
    "            cropped_Index = index[:, -Block_size:]\n",
    "            \n",
    "            #Parse the cropped indexes through the network to get the logits and loss\n",
    "            logits, loss = self(cropped_Index)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1]\n",
    "            # apply softmax to get probabilities unless you decide to use Temperature which I am\n",
    "            #logits = torch.tensor(logits, dtype=torch.long,device='cuda')\n",
    "            probs = torch.exp(logits[-1]/Temperature)/sum(torch.exp(logits[-1]/Temperature))\n",
    "            #print(torch.exp(logits[-1]/temperature))\n",
    "            #probs = F.softmax((logits.to(device)), dim=-1)\n",
    "            probs = probs.reshape(1,len(chars))\n",
    "            #print(probs)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) \n",
    "        return idx\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.895533 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = LLM()\n",
    "device = 'cuda'\n",
    "#model.load_state_dict(torch.load('./model/FromScratchModel.pth'))\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=Learning_rate) \n",
    "#print(list(model.signature.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 6.7725, val loss 6.7657\n",
      "step 50: train loss 4.3633, val loss 4.3943\n",
      "step 100: train loss 3.6019, val loss 3.6163\n",
      "step 150: train loss 3.2877, val loss 3.3044\n",
      "step 200: train loss 3.1342, val loss 3.1609\n",
      "step 250: train loss 3.0222, val loss 3.0567\n",
      "step 300: train loss 2.9005, val loss 2.9445\n",
      "step 350: train loss 2.7575, val loss 2.8173\n",
      "step 400: train loss 2.6124, val loss 2.6527\n",
      "step 450: train loss 2.4189, val loss 2.4838\n",
      "step 500: train loss 2.2689, val loss 2.3515\n",
      "step 550: train loss 2.1239, val loss 2.1850\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_51720\\728872338.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[0;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         )\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for iter in range(Max_iterations):\n",
    "   \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % Evaluation_Intervals == 0 or iter == Max_iterations - 1:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        save(model)\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    \n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
